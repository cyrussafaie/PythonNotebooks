{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we'll be looking at various kinds of neural networks.  Neural networks in python are a very quickly evolving area, and there are many different competing packages for working with them.  Unfortunately, there's not yet a standard set of packages in scikit-learn like we've seen for many other machine learning methods.  \n",
    "\n",
    "Most of the packages are high level wrappers around [Theano](http://deeplearning.net/software/theano/), which is a mathematical package for easily working with numerical expressions of arrays and matrices and their gradients.  Additionally, Theano code will also run seamlessly on a GPU if one is available.  This makes training much, much faster.  Here's an [ipython notebook](http://nbviewer.ipython.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb) on Theano if you're interested. \n",
    "\n",
    "We're going to look at two packages.  The first is [scikit-neuralnetwork](https://github.com/aigamedev/scikit-neuralnetwork) (installation instructions are at this link as well).  It's interface is the simplest, but it doesn't appear to be as widely used, and it's unclear if this package will \"win\" the race or not.\n",
    "\n",
    "[Keras](https://github.com/fchollet/keras/) is a relatively new package.  It looks to be a good balance between sophistication and simplicity.  Installation instructions are at that link.\n",
    "\n",
    "[Lasagne](http://lasagne.readthedocs.org/en/latest/) is another, more full-featured, package, but we won't have time to go into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pickle lets us save python objects to a file and read them back in\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "# here are our neural network imports\n",
    "from sknn import mlp\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import theano\n",
    "\n",
    "from urllib import urlretrieve\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# scikit-learn does have a restricted boltzman machine class for doing unsupervised\n",
    "# feature learning\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where Theano will run our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print theano.config.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had a GPU, we could use it by setting `theano.config.device='gpu'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron with a Single Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), a standard dataset of handwritten digits.  Note that this is a much bigger, higher resolution dataset than the handwritten digits dataset that we've seein in previous lectures.  In this first example, we'll use scikit-neuralnetwork and then Lasagne.  The following code is a modified version based off of the Lasagne MNIST example [here](https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll download the MNIST dataset as a pickle file, save it to a local file, and then read in its contents.  If we've already downloaded the file, we'll just read it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_URL = 'http://deeplearning.net/data/mnist/mnist.pkl.gz'\n",
    "DATA_FILENAME = 'mnist.pkl.gz'\n",
    "\n",
    "if not os.path.exists(DATA_FILENAME):\n",
    "    print \"Downloading MNIST dataset...\"\n",
    "    urlretrieve(DATA_URL, DATA_FILENAME)\n",
    "\n",
    "with gzip.open(DATA_FILENAME, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickle object has a training set, a validation set, and a test set.  Let's split those out and make a dictionary of things to pass to Theano/Lasagne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = data[0]\n",
    "X_valid, y_valid = data[1]\n",
    "X_test, y_test = data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images of handwritten digits are 28 by 28 pixels (28*28=784):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous handwritten digits dataset that we worked with was only 8 by 8 pixels.  Let's define a function so that we can look at some of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_handwritten_digit(the_image, label):\n",
    "    plt.axis('off')\n",
    "    plt.imshow(the_image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_num = 109\n",
    "plot_handwritten_digit(X_train[image_num].reshape((28, 28)), y_train[image_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define some constants that will be used in the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we'll feed in this many training examples at a time\n",
    "BATCH_SIZE = 600\n",
    "\n",
    "# this is how many times we'll go through the set of batches, i.e. a full pass over\n",
    "# all of the training data\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# number of units in the hidden layer\n",
    "NUM_HIDDEN_UNITS = 512\n",
    "\n",
    "# these parameters control the gradient descent process to learn the weights\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `scikit-neuralnetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a network with scikit-neuralnetwork because it's by far the simplest.  Unfortunately, it looks like this package only supports squared loss and not cross-entropy for classification problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sknn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers = [mlp.Layer(\"Sigmoid\", units=NUM_HIDDEN_UNITS), mlp.Layer(\"Softmax\")]\n",
    "sknn_mlp = mlp.Classifier(loss_type=\"mse\", batch_size=BATCH_SIZE, layers=layers, learning_rate=LEARNING_RATE, \n",
    "                        learning_rule=\"nesterov\", learning_momentum=MOMENTUM, n_iter=NUM_EPOCHS, verbose=True)\n",
    "sknn_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll fit it and make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sknn_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_preds = sknn_mlp.predict(X_test)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print classification_report(y_test, test_preds)\n",
    "print accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we setup the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(X_train.shape[1], NUM_HIDDEN_UNITS, init='uniform', activation='sigmoid'))\n",
    "model.add(Dense(NUM_HIDDEN_UNITS, 10, init='uniform', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=LEARNING_RATE, decay=1e-6, momentum=MOMENTUM, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keras takes a matrix of binary output labels\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, nb_epoch=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test_keras = model.predict(X_test)\n",
    "Y_test_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_preds = np_utils.categorical_probas_to_classes(Y_test_keras)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print classification_report(y_test, test_preds)\n",
    "print accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Network with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try a deeper neural network with more layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deeper_model = Sequential()\n",
    "\n",
    "deeper_model.add(Dense(X_train.shape[1], NUM_HIDDEN_UNITS, init='he_normal', activation='relu'))\n",
    "deeper_model.add(Dropout(0.5))\n",
    "deeper_model.add(Dense(NUM_HIDDEN_UNITS, NUM_HIDDEN_UNITS, init='he_normal', activation='relu'))\n",
    "deeper_model.add(Dropout(0.5))\n",
    "deeper_model.add(Dense(NUM_HIDDEN_UNITS, 10, init='he_normal', activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=4.*LEARNING_RATE, decay=0., momentum=MOMENTUM, nesterov=True)\n",
    "deeper_model.compile(loss='categorical_crossentropy', optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keras takes a matrix of binary output labels\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "deeper_model.fit(X_train, Y_train, batch_size=BATCH_SIZE, nb_epoch=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_test_keras = deeper_model.predict(X_test)\n",
    "test_preds = np_utils.categorical_probas_to_classes(Y_test_keras)\n",
    "print classification_report(y_test, test_preds)\n",
    "print accuracy_score(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsurpervised Feature Learning with a Restricted Boltzmann Machine (RBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll do unsupervised feature learning, or dimensionality reduction.  This can be done with an autoencoder or a restricted boltzmann machine (RBM).  Without getting into too many of the details, both an autoencoder and an RBM have an input and a single hidden layer of reduced dimensionality.  The hidden layer is trained so that it can take the input and reconstruct it as accurately as possible, but with a smaller set of hidden nodes than input nodes.  Luckily for us, scikit-learn has an RBM class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#digits = datasets.load_digits()\n",
    "#X_train = np.asarray(digits.data, 'float32')\n",
    "#y_train = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take our training set and center it so that the features are all between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = (X_train - np.min(X_train, 0)) / (np.max(X_train, 0) + 0.0001)  # 0-1 scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a random 10k observations to train on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_EXAMPLES = 10000\n",
    "X_train_rbm, X_test_rbm, y_train_rbm, y_test_rbm = train_test_split(X_train, y_train, test_size=0.5, random_state=0)\n",
    "\n",
    "X_train_rbm = X_train_rbm[0:N_EXAMPLES]\n",
    "y_train_rbm = y_train_rbm[0:N_EXAMPLES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBM needs a learning rate and a number of iterations to train for.  The parameter `n_components` tells it how many hidden nodes to use.  It's the dimensionality of the reduced dimension space (like PCA or t-SNE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbm = BernoulliRBM(learning_rate=0.05, n_iter=20, n_components=200, random_state=0, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the RBM like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbm.fit(X_train_rbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbm.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what kinds of features the hidden nodes are learning, we can plot each node as a 28 by 28 pixel image where the darkness is how large the weight is connecting the hidden node to the corresponding input node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4.2, 4))\n",
    "for i, comp in enumerate(rbm.components_):\n",
    "    plt.subplot(10, 20, i + 1)\n",
    "    plt.imshow(comp.reshape((28, 28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "plt.suptitle('Components extracted by the RBM', fontsize=16)\n",
    "plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look in more detail at some of the individual hidden nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_rbm_component(comp_num):\n",
    "    plt.figure(figsize=(4.2, 4))\n",
    "\n",
    "    comp = rbm.components_[comp_num]\n",
    "    plt.imshow(comp.reshape((28, 28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 60, 80, 90, 120\n",
    "plot_rbm_component(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how a logistic regression trained using just the raw pixel values does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_LOGIT_TRAIN_EXAMPLES = 4000\n",
    "\n",
    "pixel_logit = LogisticRegression()\n",
    "pixel_logit.fit(X_train_rbm[0:N_LOGIT_TRAIN_EXAMPLES], y_train_rbm[0:N_LOGIT_TRAIN_EXAMPLES])\n",
    "\n",
    "# score it on the same test set we used above\n",
    "print classification_report(y_test, pixel_logit.predict(X_test))\n",
    "print accuracy_score(y_test, pixel_logit.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how it does with the features learned by the RBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbm_logit = LogisticRegression()\n",
    "rbm_features_train = rbm.transform(X_train_rbm[0:N_LOGIT_TRAIN_EXAMPLES]) \n",
    "rbm_logit.fit(rbm_features_train, y_train_rbm[0:N_LOGIT_TRAIN_EXAMPLES])\n",
    "\n",
    "rbm_features_test = rbm.transform(X_test) \n",
    "print classification_report(y_test, rbm_logit.predict(rbm_features_test))\n",
    "print accuracy_score(y_test, rbm_logit.predict(rbm_features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take the hidden features learned by the RBM, and train a new RBM on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbm_2 = BernoulliRBM(learning_rate=0.05, n_iter=50, n_components=150, random_state=0, verbose=True)\n",
    "rbm_2.fit(rbm_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbm_logit_2 = LogisticRegression()\n",
    "rbm_features_train_2 = rbm_2.transform(rbm_features_train) \n",
    "rbm_logit_2.fit(rbm_features_train_2, y_train_rbm[0:N_LOGIT_TRAIN_EXAMPLES])\n",
    "\n",
    "rbm_features_test_2 = rbm_2.transform(rbm_features_test) \n",
    "print classification_report(y_test, rbm_logit_2.predict(rbm_features_test_2))\n",
    "print accuracy_score(y_test, rbm_logit_2.predict(rbm_features_test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
