{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent and Vowpal Wabbit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that this is **NOT** a regular ipython notebook.  It uses the bash kernel that you need to install from [here](https://github.com/takluyver/bash_kernel) to execute normal linux commands rather than just regular python.  You need ipython version 3 for it to work.  It's pretty sweet though--you can switch back and forth between executing linux commands and python commands by going to the \"Change kernel\" option in the \"Kernel\" menu above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be looking at a dataset of display ad click logs from online display advertising from Criteo.  It's a much larger version of a [Kaggle competition dataset](https://www.kaggle.com/c/criteo-display-ad-challenge).  The dataset is a terabyte and has records for 24 days, but we'll only be looking at a single day of data.  You can get the data [here](http://labs.criteo.com/downloads/download-terabyte-click-logs/).  There's also a nice [blog post](http://fastml.com/vowpal-wabbit-eats-big-data-from-the-criteo-competition-for-breakfast/) about using VW in the Criteo contest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how large the file is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls -lh data/day_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "46 GB is pretty gigantic, and clearly to large to load into my laptop's RAM.  So VW is a good option here.  Let's see how many lines (examples there are in the file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THIS WILL TAKE A LONG TIME\n",
    "wc -l data/day_0\n",
    "#195,841,983"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first two lines of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head -2 data/day_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a description of the dataset from the Criteo website:\n",
    "\n",
    "The columns are tab separated with the following schema:<br>\n",
    "&lt;label&gt; &lt;integer feature 1&gt; … &lt;integer feature 13&gt; &lt;categorical feature 1&gt; … &lt;categorical feature 26&gt;\n",
    "When a value is missing, the field is just empty.\n",
    "\n",
    "So the first field is the target value (a 1 when someone clicked on the ad, a 0 when the didn't).  Then we have numeric features, and a bunch of categorical features which we would normally need to expand out into dummies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using vowpal wabbit, which you can get from github [here](https://github.com/JohnLangford/vowpal_wabbit).  We'll be working with version 7.7, though I doubt the particular version is crucial.  Windows installation instructions here [here](https://github.com/JohnLangford/vowpal_wabbit/blob/master/README.windows.txt), though I've never tried to get it working on Windows, so YMMV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vw --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VW has a huge array of commandline options, which you can read about from the help menu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vw --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we need to do is to convert the log file into the input format that VW expects.  VW comes with a utility called `vw-csv2bin`, but we're going to write some simple python code to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tsv_to_vw(tsv_file, vw_file, skip_lines, num_lines):\n",
    "    print \"\\nTurning %s into %s...\" % (tsv_file, vw_file)\n",
    "\n",
    "    # open our input file and an output file to write to\n",
    "    with open(tsv_file, 'r') as infile, open(vw_file, 'w') as outfile:        \n",
    "        lines_read=0\n",
    "        lines_skipped=0\n",
    "        # read the file line by line\n",
    "        for line in infile:\n",
    "            # we want to skip the first skip_lines lines of the file\n",
    "            if skip_lines!=0 and lines_skipped<skip_lines:\n",
    "                lines_skipped += 1\n",
    "                continue\n",
    "  \n",
    "            # if we've converted num_lines already, stop\n",
    "            if lines_read>= num_lines: return\n",
    "\n",
    "            # othewise, convert the line\n",
    "            out_line = \"\"\n",
    "            # get rid of the newline at the end of the line\n",
    "            line = re.sub('\\n', '', line)\n",
    "            # split the file on tabs\n",
    "            data = re.split('\\t', line)\n",
    "\n",
    "            # pop off our target/label column and write the label | for vw\n",
    "            target = data.pop(0)        \n",
    "            out_line += \"1 | \" if target == \"1\" else \"-1 | \"\n",
    "\n",
    "            # write the 13 integer features in a form like feature:val, e.g. f0:124\n",
    "            for i in range(13):\n",
    "                out_line += \"f%s:\" % i\n",
    "                if data[i] == \"\":\n",
    "                    out_line += \"0 \"\n",
    "                else:\n",
    "                    out_line += \"%s \" % data[i]\n",
    "\n",
    "            # all the rest are the categorical features, so we just write these directly\n",
    "            # and vw will interpet them as F:1 when they're present, F:0 when they're not\n",
    "            for i in range(13, len(data)):\n",
    "                if data[i] == \"\": continue\n",
    "                out_line += \"f%s_%s \" % (i, data[i])\n",
    "\n",
    "            out_line += \"\\n\"\n",
    "            outfile.write(out_line)\n",
    "            lines_read += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DON'T RE-RUN THIS, BECAUSE IT WILL TAKE FOREVER...\n",
    "# also, only write out 2mm lines because my hard drive fills up...\n",
    "tsv_to_vw(\"data/day_0\", \"data/day_0.vw\", skip_lines=0, num_lines=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsv_to_vw(\"data/day_0\", \"data/day_0.test.vw\", skip_lines=2000000, num_lines=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head -10 data/day_0.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can validate the format with the super useful [VW format validator tool](http://hunch.net/~vw/validate.html\n",
    ")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll train a logistic regression model.  The `-f` flag tells it where to store the final model, and the `-d` option is for the input data.  (Note, this provides realtime feedback in the terminal that you can't see in the notebook...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vw --loss_function logistic -f data/day_0.model -d data/day_0.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of displaying the logistic loss function values, we can have it display the binary accuracy instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vw --loss_function logistic --binary -f data/day_0.model -d data/day_0.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is 94% 0's, so getting an accuracy of 97% means that it is indeed learning something about the 1's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have VW make multiple passes over the data.  In general, the more passes the better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vw --loss_function logistic --binary -f data/day_0.model -d data/day_0.vw -c --passes 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can apply the model to new data with the `-t` flag.  Predictions will be written to the file specified by the `-p` flag and raw predictions to the file specified by the `-r` flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vw -t --binary -i data/day_0.model -d data/day_0.test.vw -p data/day_0.test.preds -r data/day_0.test.raw.preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head -10 data/day_0.test.preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head -10 data/day_0.test.raw.preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a file with only the true test predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cut -d \"|\" -f 1 data/day_0.test.vw > data/day_0.test.true.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head -10 data/day_0.test.true.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"data/day_0.test.true.labels\", header=None)\n",
    "labels.columns = [\"label\"]\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = pd.read_csv(\"data/day_0.test.raw.preds\", header=None)\n",
    "preds.columns = [\"pred\"]\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(labels.label, preds.pred)\n",
    "fpr_rand = tpr_rand = np.linspace(0, 1, 10)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot(fpr_rand, tpr_rand, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roc_auc_score(labels.label, preds.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell VW to generate all quadratic interaction features and use them in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vw --loss_function logistic --binary -q aa -b 24 -d data/day_0.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with cubic features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vw --loss_function logistic --binary --cubic aaa -d data/day_0.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add lasso or ridge penalties to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vw --loss_function logistic --binary --l1 0.1 -d data/day_0.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vw --loss_function logistic --binary --l2 0.1 -d data/day_0.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had defined feature namespaces, we can mask entire chunks of features in and out of the model with the `--ignore` and `--keep` options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an idea of feature importances with the `vw-varinfo` script.  Let's first generate a tiny version of our training dataset so that this will go quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head -1000 data/day_0.vw > data/day_0.small.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vw-varinfo -d data/day_0.small.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also train a support vector machine by using the \"hinge\" loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vw --loss_function hinge --binary -f data/day_0.model -d data/day_0.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
