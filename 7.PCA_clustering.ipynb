{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups, load_digits\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# these are new imports for dimensionality reduction\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "# these are new imports for clustering\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R package [`cluster.datasets`](http://cran.r-project.org/web/packages/cluster.datasets/cluster.datasets.pdf) has some good datasets for experimenting with unsupervised learning techniques like dimensionality reduction and clustering.  Here, we'll use the `cake.ingredients.1961` dataset of cake recipes, which I've exported to a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cakes = pd.read_csv(\"./cakes.csv\")\n",
    "# strip trailing whitespace in the names\n",
    "cakes[\"Cake\"] = cakes.Cake.str.strip()\n",
    "cakes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store a dictionary of the ingredient abbreviations so we can look them up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ingredients_dict = {\n",
    "    \"AE\": \"Almond essence\",\n",
    "    \"BM\": \"Buttermilk\",\n",
    "    \"BP\": \"Baking powder\",\n",
    "    \"BR\": \"Butter\",\n",
    "    \"BS\": \"Bananas\",\n",
    "    \"CA\": \"Cocoa\",\n",
    "    \"CC\": \"Cottage Cheese\",\n",
    "    \"CE\": \"Chocolate\",\n",
    "    \"CI\": \"Crushed Ice\",\n",
    "    \"CS\": \"Crumbs\",\n",
    "    \"CT\": \"Cream of tartar\",\n",
    "    \"DC\": \"Dried currants\",\n",
    "    \"EG\": \"Eggs\",\n",
    "    \"EY\": \"Egg white\",\n",
    "    \"EW\": \"Egg yolk\",\n",
    "    \"FR\": \"Sifted flour\",\n",
    "    \"GN\": \"Gelatin\",\n",
    "    \"HC\": \"Heavy cream\",\n",
    "    \"LJ\": \"Lemon juice\",\n",
    "    \"LR\": \"Lemon\",\n",
    "    \"MK\": \"Milk\",\n",
    "    \"NG\": \"Nutmeg\",\n",
    "    \"NS\": \"Nuts\",\n",
    "    \"RM\": \"Rum\",\n",
    "    \"SA\": \"Soda\",\n",
    "    \"SC\": \"Sour cream\",\n",
    "    \"SG\": \"Shortening\",\n",
    "    \"SR\": \"Granulated sugar\",\n",
    "    \"SS\": \"Strawberries\",\n",
    "    \"ST\": \"Salt\",\n",
    "    \"VE\": \"Vanilla extract\",\n",
    "    \"WR\": \"Water\",\n",
    "    \"YT\": \"Yeast\",\n",
    "    \"ZH\": \"Zwiebach\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of the column of cake names so that we have a numeric only dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = cakes.iloc[:, 1:]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll run a simple PCA using the [scikit-learn class](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).  If we don't specify `n_components` or set it to `None`, it will use the maximum number of principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the explained variance of each of the principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\n",
    "plt.scatter(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\n",
    "plt.xlabel(\"Principal Components Number\")\n",
    "plt.ylabel(\"Percentage of Variance Explained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(range(len(pca.explained_variance_ratio_)), cumsum)\n",
    "plt.scatter(range(len(pca.explained_variance_ratio_)), cumsum)\n",
    "plt.xlabel(\"Principal Components Number\")\n",
    "plt.ylabel(\"Cumulative Percentage of Variance Explained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're looking for an \"elbow\", it looks like roughly 6 or 7 principal components would be enough.  To actually get each row transformed into the principal component space, we can call `transform` on an already fit `PCA` object, or we can do both at once with `fit_transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_trans = pca.fit_transform(X)\n",
    "X_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a function for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_PCA(pca, X, print_row_labels, row_labels, col_labels, biplot=False, y_scale=(None, None), font_size=None):\n",
    "    # transform our data to PCA space\n",
    "    X_trans = pca.fit_transform(X)\n",
    "\n",
    "    # handle the scaling of the plot\n",
    "    xmin, xmax = min(X_trans[:, 0]), max(X_trans[:, 0])\n",
    "    if y_scale == (None, None):\n",
    "        ymin, ymax = min(X_trans[:, 1]), max(X_trans[:, 1])\n",
    "        xpad, ypad = 5, 5\n",
    "    else:\n",
    "        ymin, ymax = y_scale\n",
    "        xpad, ypad = 5, 1\n",
    "        \n",
    "    plt.xlim(xmin - xpad, xmax + xpad)\n",
    "    plt.ylim(ymin - ypad, ymax + ypad)\n",
    "\n",
    "    # plot words instead of points\n",
    "    if print_row_labels:\n",
    "        for x, y, label in zip(X_trans[:, 0], X_trans[:, 1], row_labels):\n",
    "            if font_size is None:\n",
    "                plt.text(x, y, label)\n",
    "            else:\n",
    "                plt.text(x, y, label, size=font_size)\n",
    "    else:\n",
    "        for x, y in zip(X_trans[:, 0], X_trans[:, 1]):\n",
    "            plt.scatter(x, y)\n",
    "    plt.xlabel(\"PC 1\")\n",
    "    plt.ylabel(\"PC 2\")\n",
    "\n",
    "    # if we want a biplot, get the loading and plot\n",
    "    # axes with labels\n",
    "    if biplot:\n",
    "        eigenvectors = pca.components_.transpose()\n",
    "        for i, col in enumerate(col_labels):\n",
    "            x, y = 10*eigenvectors[i][0], 10*eigenvectors[i][1]\n",
    "            plt.arrow(0, 0, x, y, color='r', width=0.002, head_width=0.05)\n",
    "            plt.text(x* 1.4, y * 1.4, col, color='r', ha='center', va='center')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "plot_PCA(pca, X, True, cakes.Cake, X.columns, biplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we ran PCA on a totally unscaled version of the dataset, so we see that we're influenced by two large outliers.  The first principal component is dominated by \"cocoa\" and \"shortening\" because the \"One Bowl Chocolate\" cake has a huge amount of these.  The second principal component is dominated by \"egg whites\" because of the \"Angel\" foodcake recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cake = \"One Bowl Chocolate\"\n",
    "cake = \"Angel\"\n",
    "cakes[cakes.Cake==cake].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cakes.CA\n",
    "#cakes.SG\n",
    "cakes.EW\n",
    "#cakes.EG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try mean-centering the columns first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_scaled = scale(X, with_mean=True, with_std=False)\n",
    "plot_PCA(pca, X_scaled, True, cakes.Cake, X.columns, biplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now both center and scale to unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_scaled = scale(X, with_mean=True, with_std=True)\n",
    "plot_PCA(pca, X_scaled, True, cakes.Cake, X.columns, biplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a zoomed in version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_scaled = scale(X, with_mean=True, with_std=True)\n",
    "plot_PCA(pca, X_scaled, True, cakes.Cake, X.columns, biplot=True, y_scale=(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To me, it looks like cheesecakes are off to the right on the first principal component, and the second principal component is quantifying whether the cake has fruit or not..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch for a moment to the handwritten digits dataset that we saw before in the notebook on k-nearest neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "digits_data = scale(digits.data)\n",
    "\n",
    "labels = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "digits_trans = pca.fit_transform(digits_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a plot of the first two principal components, colored and labeled by the true digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmin, xmax = min(digits_trans[:, 0]), max(digits_trans[:, 0])\n",
    "ymin, ymax = min(digits_trans[:, 1]), max(digits_trans[:, 1])\n",
    "xpad, ypad = 5, 5\n",
    "plt.xlim(xmin - xpad, xmax + xpad)\n",
    "plt.ylim(ymin - ypad, ymax + ypad)\n",
    "\n",
    "for x, y, label in zip(digits_trans[:, 0], digits_trans[:, 1], labels):\n",
    "    plt.text(x, y, label, size=8, color=plt.cm.Set1(label/10.))\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our goal is visualizing the high dimensional dataset, the [t-SNE](http://lvdmaaten.github.io/tsne/) algorithm usually does a superior job of finding structure in the high-dimensional data that can be visualized in two dimensions.  There's a [scikit-learn class](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) for running t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=True)\n",
    "digits_trans = tsne.fit_transform(digits_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmin, xmax = min(digits_trans[:, 0]), max(digits_trans[:, 0])\n",
    "ymin, ymax = min(digits_trans[:, 1]), max(digits_trans[:, 1])\n",
    "xpad, ypad = 5, 5\n",
    "plt.xlim(xmin - xpad, xmax + xpad)\n",
    "plt.ylim(ymin - ypad, ymax + ypad)\n",
    "\n",
    "#for x, y, label in zip(digits_trans[labels==6, 0], digits_trans[labels==6, 1], labels[labels==6]):\n",
    "for x, y, label in zip(digits_trans[:, 0], digits_trans[:, 1], labels):\n",
    "    plt.text(x, y, label, size=8, color=plt.cm.Set1(label/10.))\n",
    "\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly does a better job at finding the \"structure\" in the high-dimensional dataset.  Notice that 3, 5, and 9 end up near each other.  But there are some 1's that are closer to the 2's, and some 9's that are closer to the 7's and the 1's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to cakes.  We'll use some functions from scipy to run hierarchical clustering.  `linkage` calculates the distances and linkages, and `dendrogram` displays the actual tree dendrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_single = linkage(scale(X), method='single', metric=\"euclidean\") # single, complete, average, and ward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dendr = dendrogram(clusters_single, orientation=\"top\", labels=list(cakes.Cake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As ISLR says, single linkage tends to produce really unbalanced trees.  We can put the dendrogram on its side to make it easier to visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dendr = dendrogram(clusters_single, orientation=\"right\", labels=list(cakes.Cake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_complete = linkage(scale(X), method='complete', metric=\"euclidean\") # single, complete, average, and ward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dendr = dendrogram(clusters_complete, orientation=\"top\", labels=list(cakes.Cake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dendr = dendrogram(clusters_complete, orientation=\"right\", labels=list(cakes.Cake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_ward = linkage(scale(X), method='ward', metric=\"euclidean\") # single, complete, average, and ward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dendr = dendrogram(clusters_ward, orientation=\"top\", labels=list(cakes.Cake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dendr = dendrogram(clusters_ward, orientation=\"right\", labels=list(cakes.Cake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering is doing something sensible: the cheesecakes group together and are on their own, the chocolate cakes are together (sour cream fudge, red devil's, sweet chocolate, and one bowl chocolate), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a general resource, the [scikit-learn clustering page](http://scikit-learn.org/stable/modules/clustering.html) is great.  It has all the different kinds of clustering algorithms with their pros and cons.  Here, we'll focus on k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init can be k-means++ or random; k-means++ is just a smarter version of random that forces the\n",
    "# centers to be further apart\n",
    "kmeans = KMeans(n_clusters=10, init='k-means++', n_init=10, max_iter=300, verbose=True, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.fit(digits_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the assigned cluster or label of each data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cluster centers themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"intertia\" tells us the within cluster sum-of-squares, or the \"sum of distances of samples to their closest cluster center.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we make a plot where we color by the k-means label instead of the true label.  We can see that things are decent, but definitely more confused than with the true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmin, xmax = min(digits_trans[:, 0]), max(digits_trans[:, 0])\n",
    "ymin, ymax = min(digits_trans[:, 1]), max(digits_trans[:, 1])\n",
    "xpad, ypad = 5, 5\n",
    "plt.xlim(xmin - xpad, xmax + xpad)\n",
    "plt.ylim(ymin - ypad, ymax + ypad)\n",
    "\n",
    "for x, y, true_label, kmeans_label in zip(digits_trans[:, 0], digits_trans[:, 1], labels, kmeans.labels_):\n",
    "    plt.text(x, y, true_label, size=8, color=plt.cm.Set1(kmeans_label/10.))\n",
    "\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the `predict` method, which will tell us which cluster center some new data is closest too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.predict(digits_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform` method will transform data into the cluster distance space.  That is, how far the point is from each cluster center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed = kmeans.transform(digits_data)\n",
    "transformed[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For very large datasets, there's a much faster implementation of k-means called [mini-batch k-means](http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf), and a [scikit-learn class](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html) for running it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb_kmeans = MiniBatchKMeans(n_clusters=10, batch_size=100, init='k-means++', n_init=10, max_iter=300, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb_kmeans.fit(digits_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mb_kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xmin, xmax = min(digits_trans[:, 0]), max(digits_trans[:, 0])\n",
    "ymin, ymax = min(digits_trans[:, 1]), max(digits_trans[:, 1])\n",
    "xpad, ypad = 5, 5\n",
    "plt.xlim(xmin - xpad, xmax + xpad)\n",
    "plt.ylim(ymin - ypad, ymax + ypad)\n",
    "\n",
    "for x, y, true_label, kmeans_label in zip(digits_trans[:, 0], digits_trans[:, 1], labels, mb_kmeans.labels_):\n",
    "    plt.text(x, y, true_label, size=8, color=plt.cm.Set1(kmeans_label/10.))\n",
    "\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can re-cover the \"correct\" number of clusters using the silhouette statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters = range(3, 70, 2)\n",
    "silhouette_stats = []\n",
    "for this_n_clusters in n_clusters:\n",
    "    print \"Fitting %s clusters...\" % this_n_clusters\n",
    "    kmeans = KMeans(n_clusters=this_n_clusters, init='k-means++', n_init=10, max_iter=300, verbose=False, n_jobs=1)\n",
    "    kmeans.fit(digits_data)\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_stats.append(silhouette_score(digits_data, labels, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(n_clusters, silhouette_stats)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll do dimension reduction and clustering on some text data--the 20 newsgroups dataset from last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', categories=None,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's only keep the 'rec.sport.baseball', 'rec.autos', 'sci.space', and 'talk.politics.guns' categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_keep = np.where([name in [9, 7, 14, 16] for name in data_train.target])[0]\n",
    "to_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn the blobs of text into numeric features by using tfidf, which is basically a normalized version of the word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=25, stop_words='english', use_idf=True)\n",
    "X_train = vectorizer.fit_transform(np.array(data_train.data)[to_keep])\n",
    "targets = data_train.target[to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature matrix has 2,213 postings and 1,107 features, in sparse matrix format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sparse matrices, the `TruncatedSVD` class will perform PCA much, much faster than the regular `PCA` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_trans = svd.fit_transform(X_train)\n",
    "X_train_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svd.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for four clusters in the reduced dimensionality space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=4, init_size=1000, batch_size=1000, init='k-means++', n_init=500, max_iter=1000)\n",
    "#kmeans = KMeans(n_clusters=4, init='k-means++', n_init=100, max_iter=1000)\n",
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(index=targets, columns=kmeans.labels_, rownames=['True'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the cluster centers, and find the top 10 \"directions\" or terms that correspond to each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(4):\n",
    "    print \"Cluster %d:\" % i\n",
    "    for ind in centroids[i, :10]:\n",
    "        print ' %s' % terms[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
